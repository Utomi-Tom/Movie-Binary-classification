{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMBJWac0mtk2E1oaOyHgPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utomi-Tom/Movie-Binary-classification/blob/main/IMBD_movie_classification_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The IMDB dataset\n",
        "You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the\n",
        "Internet Movie Database. They’re split into 25,000 reviews for training and 25,000\n",
        "reviews for testing, each set consisting of 50% negative and 50% positive reviews.\n",
        "\n",
        "\n",
        "Why use separate training and test sets? Because you should never test a machinelearning\n",
        "model on the same data that you used to train it! Just because a model performs\n",
        "well on its training data doesn’t mean it will perform well on data it has never\n",
        "seen; and what you care about is your model’s performance on new data (because you\n",
        "already know the labels of your training data—obviously you don’t need your model\n",
        "to predict those). For instance, it’s possible that your model could end up merely memorizing\n",
        "a mapping between your training samples and their targets, which would be\n",
        "useless for the task of predicting targets for data the model has never seen before.\n",
        "We’ll go over this point in much more detail in the next chapter.\n",
        "\n",
        "\n",
        "Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has\n",
        "already been preprocessed: the reviews (sequences of words) have been turned into\n",
        "sequences of integers, where each integer stands for a specific word in a dictionary.\n",
        "The following code will load the dataset (when you run it the first time, about\n",
        "80 MB of data will be downloaded to your machine)."
      ],
      "metadata": {
        "id": "BknqXcLKHUIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MySbF3kKGedj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset \n",
        "(x_tr, y_tr), (x_tt, y_tt) = keras.datasets.imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "7K-r3p__HK6g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The argument num_words=10000 means you’ll only keep the top 10,000 most frequently\n",
        "occurring words in the training data. Rare words will be discarded. This allows\n",
        "you to work with vector data of manageable size."
      ],
      "metadata": {
        "id": "PrfisSqfJHQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4tysPdKIMT",
        "outputId": "4974855d-7d4f-42a7-94fc-920bfab57104"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here’s how you can quickly decode one of these reviews back to English\n",
        "words:\n",
        "\n",
        "word_index is a dictionary mapping\n",
        "words to an integer index."
      ],
      "metadata": {
        "id": "-KDgRXkWKHVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_word_ind = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "metadata": {
        "id": "AlhVNY7cLpt3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reverses it, mapping\n",
        "integer indices to words"
      ],
      "metadata": {
        "id": "sAzbWv19Mr0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_review = \"\".join([r_word_ind.get(i - 3, \"?\") for i in x_tr[3]])"
      ],
      "metadata": {
        "id": "LFhyhKNeMmAL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_tr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L94_R8RfH0Jq",
        "outputId": "75a72cf8-d8f6-4e07-ff60-ba4ec7ed98fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables **x_tr and x_tt** are lists of reviews; each review is a list of\n",
        "word indices (encoding a sequence of words).**y_tr and y_tt** are\n",
        "lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
      ],
      "metadata": {
        "id": "chbHnzzwJP85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-tf0oLBI808",
        "outputId": "02a635ba-6bcd-46b5-bf4c-871311b897da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6a7iosllOfyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_review\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "m2dq6ImtNhtE",
        "outputId": "26a68b79-3d67-43cb-9d6b-05f06e2409c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"?the??atstorytellingthetraditionalsortmanyyearsaftertheeventicanstillseeinmy?eyeanelderlyladymyfriend'smotherretellingthebattleof?shemakesthecharacterscomealiveherpassionisthatofaneyewitnessonetotheeventsonthe?heathamileorsofromwhereshelivesbrbrofcourseithappenedmanyyearsbeforeshewasbornbutyouwouldn'tguessfromthewayshetellsitthesamestoryistoldinbarsthelengthand?ofscotlandasidiscusseditwithafriendonenightin?alocalcutintogivehisversionthediscussioncontinuedtoclosingtimebrbrstoriespasseddownlikethisbecomepartofourbeingwhodoesn'trememberthestoriesourparentstolduswhenwewerechildrentheybecomeourinvisibleworldandaswegrowoldertheymaybestillserveasinspirationorasanemotional?factandfictionblendwith?rolemodelswarningstories?magicandmysterybrbrmynameis?likemygrandfatherandhisgrandfatherbeforehimourprotagonistintroduceshimselftousandalsointroducesthestorythatstretchesbackthroughgenerationsitproducesstorieswithinstoriesstoriesthatevokethe?wonderofscotlanditsruggedmountains?in?thestuffoflegendyet?is?inrealitythisiswhatgivesititsspecialcharmithasaroughbeautyandauthenticity?withsomeofthefinest?singingyouwilleverhearbrbr??visitshisgrandfatherinhospitalshortlybeforehisdeathheburnswithfrustrationpartofhim?tobeinthetwentyfirstcenturytohangoutin?butheisraisedonthewestern?amonga?speakingcommunitybrbryetthereisadeeperconflictwithinhimhe?toknowthetruththetruthbehindhis?ancientstorieswheredoesfictionendandhewantstoknowthetruthbehindthedeathofhisparentsbrbrheispulledtomakealast?journeytothe?ofoneof?most?mountainscanthetruthbetoldorisitallinstoriesbrbrinthisstoryaboutstorieswe?bloodybattles?loversthe?ofoldandthesometimesmore??ofacceptedtruthindoingsoweeachconnectwith?ashelivesthestoryofhisownlifebrbr?the??isprobablythemosthonest?andgenuinelybeautifulfilmofscotlandevermadelike?igotslightlyannoyedwiththe?ofhangingstoriesonmorestoriesbutalsolike?i?thisonceisawthe?picture'forgettheboxoffice?ofbraveheartanditslikeyoumighteven?the?famous?ofthewickermantoseeafilmthatistruetoscotlandthisoneisprobablyuniqueifyoumaybe?onitdeeplyenoughyoumightevenre?thepowerofstorytellingandtheageoldquestionofwhethertherearesometruthsthatcannotbetoldbutonlyexperienced\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "GXC_V0d_N3tY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MuP8ZBSoNjl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}